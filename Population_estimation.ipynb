{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaCvdsAyM/XGZVw3+rU9LJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisahligono/GeoHUM/blob/main/Population_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample code in estimating population in Venezuela using deep learning algorithms. (Author: Lisah Khadiala Ligono) GeoHUM class assignment"
      ],
      "metadata": {
        "id": "c7QmHaz5PmQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**:\n",
        "To obtain a high-resolution satellite image of an area in Google Colab, you will need to use the **urllib** library to download the image from a remote server. \n",
        "This code will download the image from the specified URL and save it to a local file called satellite_image.jpg. It will then load the image into memory as a NumPy array using the cv2 library."
      ],
      "metadata": {
        "id": "OHwXOEPFP93N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# URL of the satellite image.\n",
        "image_url = \"https://www.example.com/satellite_image.jpg\"\n",
        "\n",
        "# Download the image and save it to a local file.\n",
        "urllib.request.urlretrieve(image_url, \"satellite_image.jpg\")\n",
        "\n",
        "# Load the image into memory as a NumPy array.\n",
        "image = cv2.imread(\"satellite_image.jpg\")"
      ],
      "metadata": {
        "id": "ppKTeavPQWu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installations\n",
        "!apt install gdal-bin python-gdal python3-gdal \n",
        "!pip install rasterio\n",
        "!apt install python3-rtree \n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "!pip install descartes\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio as rio\n",
        "from rasterio.plot import show\n",
        "\n",
        "fpath = 'http://landsat-pds.s3.amazonaws.com/c1/L8/003/065/LC08_L1TP_003065_20190925_20191017_01_T1/LC08_L1TP_003065_20190925_20191017_01_T1_B4.TIF'\n",
        "\n",
        "def rasterio_open(f):\n",
        "    return rio.open(f)\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(12, 10))\n",
        "show(src_image, ax=ax)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Xbf_yhsUvk5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: resize the image to 256x256 pixels, crop it to the area of interest, and convert it to a tensor using the torch library."
      ],
      "metadata": {
        "id": "qjr1thGHQ1kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "def preprocess_image(image):\n",
        "  # Resize the image to a smaller size, such as 256x256 pixels.\n",
        "  image = cv2.resize(image, (256, 256))\n",
        "  \n",
        "  # Crop the image to the area of interest.\n",
        "  image = image[50:200, 50:200]\n",
        "  \n",
        "  # Convert the image to a tensor.\n",
        "  image = torch.from_numpy(image).float()\n",
        "  \n",
        "  return image\n",
        "\n",
        "# Preprocess the satellite image.\n",
        "image = preprocess_image(image)"
      ],
      "metadata": {
        "id": "C3sCAFHKQ_PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** This code will use the annotation_tool library to annotate each image in the images list. For each image, it will draw bounding boxes around the buildings and label them with the class \"building\". The bounding boxes and labels are then added to the dataset list."
      ],
      "metadata": {
        "id": "7knbd3-PRD0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import annotation_tool\n",
        "\n",
        "def create_ground_truth_dataset(images):\n",
        "  dataset = []\n",
        "  \n",
        "  for image in images:\n",
        "    # Load the preprocessed image into the annotation tool.\n",
        "    # Use the annotation tool to draw bounding boxes around each building in the image.\n",
        "    # Label each bounding box with the class \"building\".\n",
        "    annotated_image = annotation_tool.annotate(image)\n",
        "    \n",
        "    # Extract the bounding boxes and labels from the annotated image.\n",
        "    boxes = annotated_image.get_boxes()\n",
        "    labels = annotated_image.get_labels()\n",
        "    \n",
        "    # Add the boxes and labels to the dataset.\n",
        "    dataset.append((boxes, labels))\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "# Create the ground truth dataset.\n",
        "dataset = create_ground_truth_dataset(images)"
      ],
      "metadata": {
        "id": "JTqgJh68RYty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**:This code will use the train_test_split function from the sklearn library to split the dataset into training and validation sets. The test_size argument specifies the proportion of the dataset to include in the validation set. In this case, the validation set will contain 20% of the data and the training set will contain the remaining 80%.\n"
      ],
      "metadata": {
        "id": "zDdOt4lwRpv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the annotated dataset into training and validation sets.\n",
        "train_set, val_set = train_test_split(dataset, test_size=0.2)"
      ],
      "metadata": {
        "id": "QHFRYAsCRvfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**:This code defines a CNN called BuildingDetector that consists of three convolutional layers, three pooling layers, and two fully connected layers. The input to the network is a 3-channel image, which is passed through the convolutional layers and pooling layers before being flattened and passed through the fully connected layers. The output of the network is a single scalar value representing the probability that a building is present in the image."
      ],
      "metadata": {
        "id": "fcE1PKaPRyHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BuildingDetector(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BuildingDetector, self).__init__()\n",
        "    \n",
        "    # Define the convolutional layers.\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "    \n",
        "    # Define the pooling layers.\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "    # Define the fully connected layers.\n",
        "    self.fc1 = nn.Linear(64 * 64 * 64, 512)\n",
        "    self.fc2 = nn.Linear(512, 1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Pass the input through the convolutional layers and pooling layers.\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    \n",
        "    # Flatten the output.\n",
        "    x = x.view(-1, 64 * 64 * 64)\n",
        "    \n",
        "    # Pass the output through the fully connected layers.\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "# Create an instance of the BuildingDetector CNN.\n",
        "model = BuildingDetector()"
      ],
      "metadata": {
        "id": "c3ygWKvmSJz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6**: This code selects the binary_cross_entropy_with_logits loss function from the torch.nn.functional library and the Adam optimizer from the torch library. The Adam optimizer is a popular choice for training deep learning models and is generally easy to use."
      ],
      "metadata": {
        "id": "kFvJzNUzSOFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Select an appropriate loss function.\n",
        "loss_fn = F.binary_cross_entropy_with_logits\n",
        "\n",
        "# Select an appropriate optimizer.\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "W8bto3xWSd9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7**: This code trains the model on the training set for 10 epochs. For each epoch, it loops over the training set and performs a forward pass through the CNN, computes the loss, and performs a backward pass to compute the gradients. It then updates the weights of the model using the optimizer."
      ],
      "metadata": {
        "id": "-Y5P0-zGShg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the number of epochs.\n",
        "num_epochs = 10\n",
        "\n",
        "# Set the model to training mode.\n",
        "model.train()\n",
        "\n",
        "# Train the model on the training set for several epochs.\n",
        "for epoch in range(num_epochs):\n",
        "  # Loop over the training set.\n",
        "  for images, labels in train_set:\n",
        "    # Preprocess the images.\n",
        "    images = preprocess_image(images)\n",
        "    \n",
        "    # Clear the gradients.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass.\n",
        "    outputs = model(images)\n",
        "    \n",
        "    # Compute the loss.\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    \n",
        "    # Backward pass.\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the weights.\n",
        "    optimizer.step()\n",
        "  \n",
        "  # Print the progress.\n",
        "  print(\"Epoch %d/%d\" % (epoch+1, num_epochs))"
      ],
      "metadata": {
        "id": "_gQWZTJVS0gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8**:This code evaluates the model on the validation set and computes the average loss and accuracy. It sets the model to evaluation mode before performing the forward pass and computes the loss using the specified loss function. It then sets the model back to training mode."
      ],
      "metadata": {
        "id": "3vmASRdGS6ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def evaluate(model, val_set, loss_fn):\n",
        "  # Set the model to evaluation mode.\n",
        "  model.eval()\n",
        "  \n",
        "  # Initialize variables to store the total loss and number of correct predictions.\n",
        "  total_loss = 0\n",
        "  num_correct = 0\n",
        "  \n",
        "  # Loop over the validation set.\n",
        "  for images, labels in val_set:\n",
        "    # Preprocess the images.\n",
        "    images = preprocess_image(images)\n",
        "    \n",
        "    # Forward pass.\n",
        "    outputs = model(images)\n",
        "    \n",
        "    # Compute the loss.\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    \n",
        "    # Update the total loss and number of correct predictions.\n",
        "    total_loss += loss.item()\n",
        "    num_correct += (torch.sigmoid(outputs) >= 0.5).squeeze().sum().item()\n",
        "  \n",
        "  # Compute the average loss and accuracy.\n",
        "  avg_loss = total_loss / len(val_set)\n",
        "  avg_acc = num_correct / len(val_set)\n",
        "  \n",
        "  # Set the model back to training mode.\n",
        "  model.train()\n",
        "  \n",
        "  return avg_loss, avg_acc\n",
        "\n",
        "# Evaluate the model on the validation set.\n",
        "avg_loss, avg_acc = evaluate(model, val_set, loss_fn)\n",
        "\n",
        "# Print the results.\n",
        "print(\"Average loss: %.4f\" % avg_loss)\n",
        "print(\"Average accuracy: %.4f\" % avg_acc)"
      ],
      "metadata": {
        "id": "shuVrU_sTPwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9**:This code loads a new satellite image and passes it through the trained CNN to predict the probability that a building is present in the image. The probability is computed using the sigmoid function from the torch library."
      ],
      "metadata": {
        "id": "2jnV_5v_TTqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "def predict(model, image):\n",
        "  # Preprocess the image.\n",
        "  image = preprocess_image(image)\n",
        "  \n",
        "  # Forward pass.\n",
        "  output = model(image)\n",
        "  \n",
        "  # Convert the output to a probability.\n",
        "  prob = torch.sigmoid(output).item()\n",
        "  \n",
        "  return prob\n",
        "\n",
        "# Load a new satellite image.\n",
        "image = cv2.imread(\"new_image.jpg\")\n",
        "\n",
        "# Predict the presence of buildings in the image.\n",
        "prob = predict(model, image)\n",
        "\n",
        "print(\"Probability of building: %.4f\" % prob)"
      ],
      "metadata": {
        "id": "msohs7fgTj0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10**:This code uses the trained CNN to predict the number of buildings in the satellite image and then uses the predict method of the regressor object to predict the population based on the number of buildings. The regressor object could be a trained linear regression model, for example.\n",
        "\n"
      ],
      "metadata": {
        "id": "aZO7t-sLTuFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def estimate_population(model, image, regressor):\n",
        "  # Predict the number of buildings in the image.\n",
        "  prob = predict(model, image)\n",
        "  num_buildings = prob * 1000\n",
        "  \n",
        "  # Use the regressor to predict the population based on the number of buildings.\n",
        "  population = regressor.predict(num_buildings)\n",
        "  \n",
        "  return population\n",
        "\n",
        "# Load a new satellite image.\n",
        "image = cv2.imread(\"new_image.jpg\")\n",
        "\n",
        "# Estimate the population of the area in the image.\n",
        "population = estimate_population(model, image, regressor)\n",
        "\n",
        "print(\"Estimated population: %d\" % population)\n"
      ],
      "metadata": {
        "id": "nsGbwmBMUGXD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}